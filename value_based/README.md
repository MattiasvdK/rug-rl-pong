[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-24ddc0f5d75046c5622901739e7c5dd533143b0c8e959d652212380cedb1ea36.svg)](https://classroom.github.com/a/bWgJkOvt)
# Game of Catch

In this assignment, you will implement various RL algorithms that learn the game of Catch. For exact description, refer to the assignment description on Brightspace.

## Installation

Before running the code, make sure you properly install the dependencies:

```
python3 -m pip install -r requirements.txt
```

The code has been tested with Python 3.11.

## Running the code

Now, you should be able to simply run the `main.py` as any other script.

In order to allow us for easy grading, we implemented a simple test that checks wheter your `requirements.txt` file is complete and that your code runs without errors. You can run this test by executing the following command:

```
source test.sh
```

## Tips and Resources

Here are a couple of hints and resources that might help you with in this assignment:

1. To help you out with technical writing, check out these papers for inspiration. Reading real scientific papers can help you out with using correct nomenclature and ensuring a clear structure. In particular, you can draw inspiration as to how complex concepts and formulas are introduced
   and explained.

   a. Technical Report on implementing RL algorithms in CartPole environment - https://arxiv.org/pdf/2006.04938.pdf

   b. Paper summarising usage of RL in Chess - https://page.mi.fu-berlin.de/block/concibe2008.pdf

2. If you have duplicate code in multiple places, itâ€™s probably a bad sign. Maybe you should try it to group that functionality in a seperate function?
3. The agent should be able to learn using different types of algorithms. Maybe there is a way to make these algorithms easily swappable?
4. Type hinting is not required, but it can help your partner understand your code - https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html
5. Git workshop by Cover - https://studysupport.svcover.nl/?a=1
6. YouTube Git tutorial - https://www.youtube.com/watch?v=RGOj5yH7evk
7. OOP in Python - https://www.youtube.com/watch?v=JeznW_7DlB0
8. How to document Python? - https://www.datacamp.com/tutorial/docstrings-python4

## Questions and help

If you are struggling with one part of the assignment, you're probably not alone. That's why we want to create a small FAQ throughout the next couple of weeks. In case of a question, raise an issue in the original, template repository: [https://github.com/Deep-Reinforcement-Learning-RUG/catch-assignment](https://github.com/Deep-Reinforcement-Learning-RUG/catch-assignment). We will answer your questions there, so that there are no duplicate questions.

## Running the Code

The code can be run through calling the main function. To run different agents, the name of the
agent to the arguments:

```
python3 ./main.py <agent>
```

e.g.:

```
python3 ./main.py dqn
```

If no agent name is provided the `dqn` agent is used.

The agent names are:
* dqn: Deep Q-Learning (`dqnagent.py`)
* ddqn: Double Deep Q-Learning (`ddqnagent.py`)
* duel: Dueling Architecture (`duelingagent.py`)
* dqv: Deep Quality-Value Learning (`dqvagent.py`)
* dqvm: Deep Quality-Value-Max Learning (`dqvmagent.py`)

**It is important to note that running an agent overwrites the results written for that agent.
The program will ask for confirmation before overwriting the results.**

## Relevant Changes

I changed the structure of the code to a certain degree. The basic logic of running the environment remains the same but
it is now encapsuled in the simulator class.

The main scripts runs the simulator together with the agent factory.
The agent factory implements a grid-search over the hyperparameters defined
in `hyperparameters.py`.

## Important Added Classes

AgentFactory (`agentfactory.py`): Generates instances of the provided Agent class based on grid-search over the provided parameters in `hyperparameters.py`.

Simulator (`simulator.py`): Runs the agents generated by the AgentFactory for a given number of simulations and episodes.
It calculates the results of terminal states and logs those through the CSVWriter class

CSVWriter (`csvwriter.py`): Writes the logged results and hyperparameters to csv files in `./data/`
Hyperparameters are logged in `./data/<agent>_params.csv` while the results for each different instance
is logged in the files `./data/<agent>_results_<identifier>.csv` where the identifier is a unique index
of the agent with the given hyperparameters. The results of an agent defined in the hyperparameter file
can thus be linked through the identifier.

## Some Notes

* The DQVAgent duplicates quite some code from the DQNAgent, which could probably be reused instead. Sadly I did not have time
to refactor the DQAgent class once I realized.

  